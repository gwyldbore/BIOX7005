{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99f149b",
   "metadata": {},
   "source": [
    "## This notebook takes a FASTA file and saves them to a dataframe with embeddings - in embedding_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "978935cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ete3 import Tree\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import random\n",
    "from Bio import SeqIO, AlignIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef29abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_df(\n",
    "    *fasta_paths,\n",
    "    drop_duplicates=True,\n",
    "    alignment=False,\n",
    "    ancestor=False,\n",
    "    alphabet=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ-\",\n",
    "):\n",
    "    seq_list = []\n",
    "    duplicates = {}\n",
    "\n",
    "    cols = [\n",
    "        \"info\",\n",
    "        \"truncated_info\",\n",
    "        \"extracted_id\",\n",
    "        \"extracted_name\",\n",
    "        \"sequence\",\n",
    "        \"original_fasta\",\n",
    "    ]\n",
    "\n",
    "    if alignment or ancestor:\n",
    "        print(\"This is an alignment\")\n",
    "        cols.append(\"original_alignment\")\n",
    "        cols.append(\"Sequence_aligned\")\n",
    "\n",
    "    # if ancestor:\n",
    "    #     cols.append(\"Sequence_aligned\")\n",
    "\n",
    "    for fasta_path in fasta_paths:\n",
    "        # Load FASTA file\n",
    "        # seqs = sequence.readFastaFile(fasta_path, alpha)\n",
    "\n",
    "        if alignment:\n",
    "            seqs = AlignIO.parse(open(fasta_path), format=\"fasta\")\n",
    "\n",
    "        else:\n",
    "            seqs = SeqIO.parse(open(fasta_path), format=\"fasta\")\n",
    "\n",
    "        # Add to annotation file\n",
    "        for seq in seqs:\n",
    "            if alignment == False:\n",
    "                if seq.name in duplicates:\n",
    "                    print(\n",
    "                        f\"DUPLICATE:{seq.name} is in {duplicates[seq.name]} and {fasta_path}\\n\"\n",
    "                    )\n",
    "                else:\n",
    "                    duplicates[seq.name] = fasta_path\n",
    "\n",
    "                curr_seq = [\n",
    "                    seq.id,\n",
    "                    seq.id.split(\" \")[0],\n",
    "                    seq.id.split(\"|\")[1]\n",
    "                    if len(seq.id.split(\"|\")) > 1\n",
    "                    else seq.id.split(\" \")[0],\n",
    "                    seq.id.split(\"|\")[-1],\n",
    "                    \"\".join(str(seq.seq).replace(\"-\", \"\"))\n",
    "                    if len(seq.seq) > 0\n",
    "                    else None,\n",
    "                    fasta_path,\n",
    "                ]\n",
    "\n",
    "                seq_list.append(curr_seq)\n",
    "\n",
    "            elif alignment:\n",
    "                for aligned_seq in seq:\n",
    "                    curr_seq = [\n",
    "                        aligned_seq.id,\n",
    "                        aligned_seq.id.split(\" \")[0],\n",
    "                        aligned_seq.id.split(\"|\")[1]\n",
    "                        if len(aligned_seq.id.split(\"|\")) > 1\n",
    "                        else aligned_seq.id.split(\" \")[0],\n",
    "                        aligned_seq.id.split(\"|\")[-1],\n",
    "                        \"\".join(str(aligned_seq.seq).replace(\"-\", \"\"))\n",
    "                        if len(aligned_seq.seq) > 0\n",
    "                        else None,\n",
    "                        None,\n",
    "                        fasta_path,\n",
    "                        \"\".join(aligned_seq.seq),\n",
    "                    ]\n",
    "                    seq_list.append(curr_seq)\n",
    "\n",
    "            # if ancestor:\n",
    "            #     curr_seq.append(\"\".join(aligned_seq.seq))\n",
    "\n",
    "    df = pd.DataFrame(seq_list, columns=cols)\n",
    "\n",
    "    if drop_duplicates:\n",
    "        df = df.drop_duplicates(subset=\"info\", keep=\"first\")\n",
    "\n",
    "    # Drop the sequence column if there are no sequences (i.e. if we just added a list of identifiers)\n",
    "    nan_value = float(\"NaN\")\n",
    "\n",
    "    # df.replace(\"\", nan_value, inplace=True)\n",
    "\n",
    "    df.dropna(how=\"all\", axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a688236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_embeddings(sequence, model, tokenizer, model_type):\n",
    "    \"\"\"Calculate various embeddings for a given sequence.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        \" \".join(sequence), return_tensors=\"pt\", padding=True, truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        if model_type == \"protbert\":\n",
    "            outputs = model(**inputs)\n",
    "        elif model_type == \"t5\":\n",
    "            outputs = model(**inputs.input_ids)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Mean pooling\n",
    "    mean_embedding = embeddings.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    # CLS token pooling\n",
    "    cls_embedding = embeddings[:, 0].squeeze().numpy()\n",
    "\n",
    "    # Max pooling\n",
    "    max_embedding = embeddings.max(dim=1).values.squeeze().numpy()\n",
    "\n",
    "    # Weighted pooling\n",
    "    weights = torch.linspace(0.1, 1.0, embeddings.size(1), device=embeddings.device)\n",
    "    weights = weights.unsqueeze(0).unsqueeze(\n",
    "        -1\n",
    "    )  # Add extra dimensions for broadcasting\n",
    "    weighted_embedding = (embeddings * weights).mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    return {\n",
    "        f\"{model_type}_mean_embedding\": mean_embedding,\n",
    "        f\"{model_type}_cls_embedding\": cls_embedding,\n",
    "        f\"{model_type}_max_embedding\": max_embedding,\n",
    "        f\"{model_type}_weighted_embedding\": weighted_embedding,\n",
    "    }\n",
    "\n",
    "\n",
    "def process_and_store_embeddings(df, model_name, embedding_df_path, model_type):\n",
    "    \"\"\"Process and store multiple types of embeddings for sequences in the DataFrame.\"\"\"\n",
    "    model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Load existing embeddings if they exist\n",
    "    if os.path.exists(embedding_df_path):\n",
    "        embedding_df = pd.read_pickle(embedding_df_path)\n",
    "    else:\n",
    "        embedding_df = pd.DataFrame(columns=[\"info\", \"sequence\", \"model_name\"])\n",
    "        \n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        info = row[\"info\"]\n",
    "        sequence = row[\"sequence\"]\n",
    "\n",
    "        existing_row = embedding_df[\n",
    "            (embedding_df[\"info\"] == info)\n",
    "            & (embedding_df[\"model_name\"] == model_name)\n",
    "        ]\n",
    "\n",
    "        # if not existing_row.empty:\n",
    "        \n",
    "        if not existing_row.empty and f\"{model_type}_mean_embedding\" in existing_row.columns:\n",
    "            \n",
    "               # Ensure the specific column has data\n",
    "            if not existing_row[f\"{model_type}_mean_embedding\"].empty:\n",
    "                continue  # Skip if embeddings for this sequence already exist\n",
    "\n",
    "\n",
    "        try:\n",
    "            embeddings = calculate_embeddings(sequence, model, tokenizer, model_type)\n",
    "            new_row = {\n",
    "                \"info\": info,\n",
    "                \"sequence\": sequence,\n",
    "                \"model_name\": model_name,\n",
    "                **embeddings,\n",
    "            }\n",
    "            embedding_df = pd.concat(\n",
    "                [embedding_df, pd.DataFrame([new_row])], ignore_index=True\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process sequence {sequence} with error: {e}\")\n",
    "\n",
    "    # Save embedding_df with full embeddings\n",
    "    embedding_df.to_pickle(embedding_df_path)\n",
    "    merged_df = pd.merge(df, embedding_df, on=['info', 'sequence'], how='left')\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "992501f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an alignment\n",
      "This is an alignment\n"
     ]
    }
   ],
   "source": [
    "df = get_sequence_df(\"./NR_MSA_ancestors.fa\", alignment=True)\n",
    "df_extant = get_sequence_df(\"../NR_MSA.fasta\", alignment=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff3379ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>info</th>\n",
       "      <th>truncated_info</th>\n",
       "      <th>extracted_id</th>\n",
       "      <th>extracted_name</th>\n",
       "      <th>sequence</th>\n",
       "      <th>original_alignment</th>\n",
       "      <th>Sequence_aligned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16240</th>\n",
       "      <td>Podarcis_ESRRA_40-2641-225</td>\n",
       "      <td>Podarcis_ESRRA_40-2641-225</td>\n",
       "      <td>Podarcis_ESRRA_40-2641-225</td>\n",
       "      <td>Podarcis_ESRRA_40-2641-225</td>\n",
       "      <td>NTMVSHLLVAEPEKLYAMPDPALPDSPAKAASTLCDLADREIVVII...</td>\n",
       "      <td>../NR_MSA.fasta</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16241</th>\n",
       "      <td>Petromyzon_ESRRB_82-3051-224</td>\n",
       "      <td>Petromyzon_ESRRB_82-3051-224</td>\n",
       "      <td>Petromyzon_ESRRB_82-3051-224</td>\n",
       "      <td>Petromyzon_ESRRB_82-3051-224</td>\n",
       "      <td>NKMVSQLLVVEPDRLFAMAGPGAAECDVTALTTLCDLADRELVLII...</td>\n",
       "      <td>../NR_MSA.fasta</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16242</th>\n",
       "      <td>Ochotona_ESRRB_111-3341-224</td>\n",
       "      <td>Ochotona_ESRRB_111-3341-224</td>\n",
       "      <td>Ochotona_ESRRB_111-3341-224</td>\n",
       "      <td>Ochotona_ESRRB_111-3341-224</td>\n",
       "      <td>TKIVSCLMVAEPNNLQAMPPAGIPEADIKALATLCDLADRELVVII...</td>\n",
       "      <td>../NR_MSA.fasta</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16243</th>\n",
       "      <td>Rana_ESRRB_130-3531-224</td>\n",
       "      <td>Rana_ESRRB_130-3531-224</td>\n",
       "      <td>Rana_ESRRB_130-3531-224</td>\n",
       "      <td>Rana_ESRRB_130-3531-224</td>\n",
       "      <td>TRIVSHLLLAEPEKIFAMADPAGPDSDIKVLSTLVDLTDRELVMTI...</td>\n",
       "      <td>../NR_MSA.fasta</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16244</th>\n",
       "      <td>Pipistrellus_ESRRB_147-371-224</td>\n",
       "      <td>Pipistrellus_ESRRB_147-371-224</td>\n",
       "      <td>Pipistrellus_ESRRB_147-371-224</td>\n",
       "      <td>Pipistrellus_ESRRB_147-371-224</td>\n",
       "      <td>TKIVSYLLVAEPNKPSARPPPGMPESDIKALTTLCDLADQELVSII...</td>\n",
       "      <td>../NR_MSA.fasta</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 info                  truncated_info  \\\n",
       "16240      Podarcis_ESRRA_40-2641-225      Podarcis_ESRRA_40-2641-225   \n",
       "16241    Petromyzon_ESRRB_82-3051-224    Petromyzon_ESRRB_82-3051-224   \n",
       "16242     Ochotona_ESRRB_111-3341-224     Ochotona_ESRRB_111-3341-224   \n",
       "16243         Rana_ESRRB_130-3531-224         Rana_ESRRB_130-3531-224   \n",
       "16244  Pipistrellus_ESRRB_147-371-224  Pipistrellus_ESRRB_147-371-224   \n",
       "\n",
       "                         extracted_id                  extracted_name  \\\n",
       "16240      Podarcis_ESRRA_40-2641-225      Podarcis_ESRRA_40-2641-225   \n",
       "16241    Petromyzon_ESRRB_82-3051-224    Petromyzon_ESRRB_82-3051-224   \n",
       "16242     Ochotona_ESRRB_111-3341-224     Ochotona_ESRRB_111-3341-224   \n",
       "16243         Rana_ESRRB_130-3531-224         Rana_ESRRB_130-3531-224   \n",
       "16244  Pipistrellus_ESRRB_147-371-224  Pipistrellus_ESRRB_147-371-224   \n",
       "\n",
       "                                                sequence original_alignment  \\\n",
       "16240  NTMVSHLLVAEPEKLYAMPDPALPDSPAKAASTLCDLADREIVVII...    ../NR_MSA.fasta   \n",
       "16241  NKMVSQLLVVEPDRLFAMAGPGAAECDVTALTTLCDLADRELVLII...    ../NR_MSA.fasta   \n",
       "16242  TKIVSCLMVAEPNNLQAMPPAGIPEADIKALATLCDLADRELVVII...    ../NR_MSA.fasta   \n",
       "16243  TRIVSHLLLAEPEKIFAMADPAGPDSDIKVLSTLVDLTDRELVMTI...    ../NR_MSA.fasta   \n",
       "16244  TKIVSYLLVAEPNKPSARPPPGMPESDIKALTTLCDLADQELVSII...    ../NR_MSA.fasta   \n",
       "\n",
       "                                        Sequence_aligned  \n",
       "16240  ----------------------------------------------...  \n",
       "16241  ----------------------------------------------...  \n",
       "16242  ----------------------------------------------...  \n",
       "16243  ----------------------------------------------...  \n",
       "16244  ----------------------------------------------...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.tail()\n",
    "df_extant.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b970f3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at yarongef/DistilProtBert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/georgiawyldbore/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at yarongef/DistilProtBert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/georgiawyldbore/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m bert_embedding_df_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./protbert_embeddings_NR.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m embedding_df \u001b[38;5;241m=\u001b[39m process_and_store_embeddings(df, bert_model_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./embdding_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotbert\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m embedding_extant_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_and_store_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_extant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./embedding_extant_df.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprotbert\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 71\u001b[0m, in \u001b[0;36mprocess_and_store_embeddings\u001b[0;34m(df, model_name, embedding_df_path, model_type)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip if embeddings for this sequence already exist\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     new_row \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m\"\u001b[39m: info,\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sequence,\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name,\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39membeddings,\n\u001b[1;32m     77\u001b[0m     }\n\u001b[1;32m     78\u001b[0m     embedding_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[1;32m     79\u001b[0m         [embedding_df, pd\u001b[38;5;241m.\u001b[39mDataFrame([new_row])], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mcalculate_embeddings\u001b[0;34m(sequence, model, tokenizer, model_type)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotbert\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 8\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     10\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs\u001b[38;5;241m.\u001b[39minput_ids)\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    574\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    524\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:394\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    383\u001b[0m         hidden_states,\n\u001b[1;32m    384\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m         output_attentions,\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    392\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 394\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/BIOX7005/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_model_name = \"yarongef/DistilProtBert\"\n",
    "bert_embedding_df_path = \"./protbert_embeddings_NR.pkl\"\n",
    "\n",
    "embedding_df = process_and_store_embeddings(df, bert_model_name, \"./embdding_df.csv\", model_type='protbert')\n",
    "# embedding_extant_df = process_and_store_embeddings(df_extant, bert_model_name, \"./embedding_extant_df.csv\", model_type='protbert')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890d3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>info</th>\n",
       "      <th>truncated_info</th>\n",
       "      <th>extracted_id</th>\n",
       "      <th>extracted_name</th>\n",
       "      <th>sequence</th>\n",
       "      <th>original_alignment</th>\n",
       "      <th>Sequence_aligned</th>\n",
       "      <th>model_name</th>\n",
       "      <th>protbert_mean_embedding</th>\n",
       "      <th>protbert_cls_embedding</th>\n",
       "      <th>protbert_max_embedding</th>\n",
       "      <th>protbert_weighted_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N0</td>\n",
       "      <td>N0</td>\n",
       "      <td>N0</td>\n",
       "      <td>N0</td>\n",
       "      <td>TCAKLEPEDADENIDVTGNEPERTSTEYQMSPYPSASPESVYETSA...</td>\n",
       "      <td>./NR_MSA_ancestors.fa</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>yarongef/DistilProtBert</td>\n",
       "      <td>[-0.17380047, 0.037478294, 0.049007155, -0.013...</td>\n",
       "      <td>[0.037427068, 0.075575374, -0.036185384, -0.23...</td>\n",
       "      <td>[0.12167235, 0.28130862, 0.21617526, 0.2462811...</td>\n",
       "      <td>[-0.10807977, 0.01987329, 0.028579468, -0.0042...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N1</td>\n",
       "      <td>N1</td>\n",
       "      <td>N1</td>\n",
       "      <td>N1</td>\n",
       "      <td>TCAKLEPEDADENIDVTGNEPERTSTEYQMSPYPSASPESVYETSA...</td>\n",
       "      <td>./NR_MSA_ancestors.fa</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>yarongef/DistilProtBert</td>\n",
       "      <td>[-0.17380047, 0.037478294, 0.049007155, -0.013...</td>\n",
       "      <td>[0.037427068, 0.075575374, -0.036185384, -0.23...</td>\n",
       "      <td>[0.12167235, 0.28130862, 0.21617526, 0.2462811...</td>\n",
       "      <td>[-0.10807977, 0.01987329, 0.028579468, -0.0042...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N2</td>\n",
       "      <td>N2</td>\n",
       "      <td>N2</td>\n",
       "      <td>N2</td>\n",
       "      <td>TCAKLEPEDADENIDVTGNEPERTSTEYQMSPYPSASPESVYETSA...</td>\n",
       "      <td>./NR_MSA_ancestors.fa</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>yarongef/DistilProtBert</td>\n",
       "      <td>[-0.17380047, 0.037478294, 0.049007155, -0.013...</td>\n",
       "      <td>[0.037427068, 0.075575374, -0.036185384, -0.23...</td>\n",
       "      <td>[0.12167235, 0.28130862, 0.21617526, 0.2462811...</td>\n",
       "      <td>[-0.10807977, 0.01987329, 0.028579468, -0.0042...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N3</td>\n",
       "      <td>N3</td>\n",
       "      <td>N3</td>\n",
       "      <td>N3</td>\n",
       "      <td>TCAKLEPEDADENIDVTGNEPERTSTEYPMSPYPSASPESVYETSA...</td>\n",
       "      <td>./NR_MSA_ancestors.fa</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>yarongef/DistilProtBert</td>\n",
       "      <td>[-0.17028484, 0.036823433, 0.049609266, -0.010...</td>\n",
       "      <td>[0.04245688, 0.08160813, -0.030255817, -0.2466...</td>\n",
       "      <td>[0.11976309, 0.27247024, 0.21556818, 0.2510118...</td>\n",
       "      <td>[-0.10674974, 0.019780792, 0.028877182, -0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N4</td>\n",
       "      <td>N4</td>\n",
       "      <td>N4</td>\n",
       "      <td>N4</td>\n",
       "      <td>TCAKLEPEDADENIDVTGNEPERTSTEYPMSPYPSASPEGVYETSA...</td>\n",
       "      <td>./NR_MSA_ancestors.fa</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>yarongef/DistilProtBert</td>\n",
       "      <td>[-0.1697389, 0.035590477, 0.050194185, -0.0116...</td>\n",
       "      <td>[0.040663913, 0.08406882, -0.034715842, -0.248...</td>\n",
       "      <td>[0.122089, 0.27262154, 0.21623468, 0.24625377,...</td>\n",
       "      <td>[-0.10627797, 0.01953295, 0.029021893, -0.0033...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  info truncated_info extracted_id extracted_name  \\\n",
       "0   N0             N0           N0             N0   \n",
       "1   N1             N1           N1             N1   \n",
       "2   N2             N2           N2             N2   \n",
       "3   N3             N3           N3             N3   \n",
       "4   N4             N4           N4             N4   \n",
       "\n",
       "                                            sequence     original_alignment  \\\n",
       "0  TCAKLEPEDADENIDVTGNEPERTSTEYQMSPYPSASPESVYETSA...  ./NR_MSA_ancestors.fa   \n",
       "1  TCAKLEPEDADENIDVTGNEPERTSTEYQMSPYPSASPESVYETSA...  ./NR_MSA_ancestors.fa   \n",
       "2  TCAKLEPEDADENIDVTGNEPERTSTEYQMSPYPSASPESVYETSA...  ./NR_MSA_ancestors.fa   \n",
       "3  TCAKLEPEDADENIDVTGNEPERTSTEYPMSPYPSASPESVYETSA...  ./NR_MSA_ancestors.fa   \n",
       "4  TCAKLEPEDADENIDVTGNEPERTSTEYPMSPYPSASPEGVYETSA...  ./NR_MSA_ancestors.fa   \n",
       "\n",
       "                                    Sequence_aligned               model_name  \\\n",
       "0  ----------------------------------------------...  yarongef/DistilProtBert   \n",
       "1  ----------------------------------------------...  yarongef/DistilProtBert   \n",
       "2  ----------------------------------------------...  yarongef/DistilProtBert   \n",
       "3  ----------------------------------------------...  yarongef/DistilProtBert   \n",
       "4  ----------------------------------------------...  yarongef/DistilProtBert   \n",
       "\n",
       "                             protbert_mean_embedding  \\\n",
       "0  [-0.17380047, 0.037478294, 0.049007155, -0.013...   \n",
       "1  [-0.17380047, 0.037478294, 0.049007155, -0.013...   \n",
       "2  [-0.17380047, 0.037478294, 0.049007155, -0.013...   \n",
       "3  [-0.17028484, 0.036823433, 0.049609266, -0.010...   \n",
       "4  [-0.1697389, 0.035590477, 0.050194185, -0.0116...   \n",
       "\n",
       "                              protbert_cls_embedding  \\\n",
       "0  [0.037427068, 0.075575374, -0.036185384, -0.23...   \n",
       "1  [0.037427068, 0.075575374, -0.036185384, -0.23...   \n",
       "2  [0.037427068, 0.075575374, -0.036185384, -0.23...   \n",
       "3  [0.04245688, 0.08160813, -0.030255817, -0.2466...   \n",
       "4  [0.040663913, 0.08406882, -0.034715842, -0.248...   \n",
       "\n",
       "                              protbert_max_embedding  \\\n",
       "0  [0.12167235, 0.28130862, 0.21617526, 0.2462811...   \n",
       "1  [0.12167235, 0.28130862, 0.21617526, 0.2462811...   \n",
       "2  [0.12167235, 0.28130862, 0.21617526, 0.2462811...   \n",
       "3  [0.11976309, 0.27247024, 0.21556818, 0.2510118...   \n",
       "4  [0.122089, 0.27262154, 0.21623468, 0.24625377,...   \n",
       "\n",
       "                         protbert_weighted_embedding  \n",
       "0  [-0.10807977, 0.01987329, 0.028579468, -0.0042...  \n",
       "1  [-0.10807977, 0.01987329, 0.028579468, -0.0042...  \n",
       "2  [-0.10807977, 0.01987329, 0.028579468, -0.0042...  \n",
       "3  [-0.10674974, 0.019780792, 0.028877182, -0.003...  \n",
       "4  [-0.10627797, 0.01953295, 0.029021893, -0.0033...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_df.head()\n",
    "embedding_extant_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIOX7005",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
