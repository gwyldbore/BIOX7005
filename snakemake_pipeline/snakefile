try:
    NUM_REPLICATES = config['num_replicates']
except KeyError:
    NUM_REPLICATES = 50

# try:
#     DATASET_PATH = config['dataset_path']
# except KeyError:
#     DATASET_PATH = 'data'

# # for testing purposes
# try: 
#     DATASET_PATH = config['dataset_path']
# except KeyError:
#     DATASET_PATH = 'data/cdhit_70/grasp_marginal'

# actual path
try:
    DATASET_PATH = config['dataset_path']
except KeyError:
    DATASET_PATH = 'data/reportdata'

try:
    ALL_DATASET_NAMES = config['all_dataset_names']
except KeyError:
    # ALL_DATASET_NAMES = ['NR4_NR1_ancestors_unaligned']
    # ALL_DATASET_NAMES = ['cd70_NR1toNR4_N6_N81', 'cd70_NR4toNR1_N81_N6', 'cd80_NR1toNR4_N7_N186', 'cd80_NR4toNR1_N186_N7', 'cd85_NR1toNR4_N7_N299', 'cd85_NR4toNR1_N299_N7']
    # ALL_DATASET_NAMES = ['cd70', 'cd80', 'cd85']
    # ALL_DATASET_NAMES = ['cd70']
    # ALL_DATASET_NAMES = ['cd80']
    ALL_DATASET_NAMES = ['cd85']

try:
    ALL_DATAFILE_NAMES = config['all_datafile_names']
except KeyError:
    # ALL_DATAFILE_NAMES = ['NR1toNR4_N6_N81', 'NR4toNR1_N81_N6']
    # ALL_DATAFILE_NAMES = ['NR1toNR4_N7_N186', 'NR4toNR1_N186_N7']
    ALL_DATAFILE_NAMES = ['NR1toNR4_N7_N299', 'NR4toNR1_N299_N7']

try:
    INTERPROSCAN_DIR = config['interproscan_dir']
except KeyError:
    INTERPROSCAN_DIR = '/media/WorkingSpace/Share/interproscan/interproscan-5.65-97.0'

try:
    METHOD_NAMES = config['method_names']
except KeyError:
    METHOD_NAMES = ['random', 'nonconservative', 'marginal_weights']


rule all:
    input:
        statsplot= expand("workflows/{dataset_name}/{method_name}/results/{datafile_name}_{graph_type}.png",
        dataset_name=ALL_DATASET_NAMES,
        datafile_name=ALL_DATAFILE_NAMES,
        method_name=METHOD_NAMES,
        graph_type=['mutation', 'position']
        ),

        plot_pca= expand("workflows/{dataset_name}/{method_name}/plots/{datafile_name}_{rep}_mutations.png",
        dataset_name=ALL_DATASET_NAMES,
        datafile_name=ALL_DATAFILE_NAMES,
        method_name=METHOD_NAMES,
        rep=range(1, NUM_REPLICATES + 1, 10),
        ),

        plot_pca_grouped = expand("workflows/{dataset_name}/{method_name}/plots/{datafile_name}_{batch}_mutations.png",
        dataset_name=ALL_DATASET_NAMES,
        datafile_name=ALL_DATAFILE_NAMES,
        method_name=METHOD_NAMES,
        batch=range((NUM_REPLICATES + 4) // 5),
        ),



        # # interproscan_df = expand("workflows/{dataset_name}/interproscan/{dataset_name}_{rep}.csv",
        # interproscan_df = expand("workflows/{dataset_name}/{method_name}/interproscan/{datafile_name}_{rep}.csv",
        # dataset_name=ALL_DATASET_NAMES,
        # datafile_name=ALL_DATAFILE_NAMES,
        # method_name=METHOD_NAMES,
        # rep=range(1, NUM_REPLICATES + 1),
        # ),

        # # merged_df= expand("workflows/{dataset_name}/merged/{dataset_name}_{rep}.csv",
        # merged_df= expand("workflows/{dataset_name}/{method_name}/merged/{datafile_name}_{rep}.csv",
        # dataset_name=ALL_DATASET_NAMES,
        # datafile_name=ALL_DATAFILE_NAMES,
        # method_name=METHOD_NAMES,
        # rep=range(1, NUM_REPLICATES + 1),
        # ),        

        # predict= expand("workflows/{dataset_name}/{method_name}/results/{datafile_name}_{rep}.csv",
        # dataset_name=ALL_DATASET_NAMES,
        # datafile_name=ALL_DATAFILE_NAMES,
        # method_name=METHOD_NAMES,
        # rep=range(1, NUM_REPLICATES + 1),
        # ),

        # statplots= expand("workflows/{dataset_name}/results/{datafile_name}.png",
        # dataset_name=ALL_DATASET_NAMES,
        # datafile_name=ALL_DATAFILE_NAMES,
        # ),

#         generate_sequences = expand(
#         "workflows/{dataset_name}/fasta/{dataset_name}_{rep}.fasta",
#         dataset_name=ALL_DATASET_NAMES,
#         rep=range(1, NUM_REPLICATES + 1)
#         ),



rule generate_ancestor_embeddings:
    input:
        input_sequences=DATASET_PATH + "/{dataset_name}_ancestors.fa"
    output:
        embedding_df="workflows/{dataset_name}/embeddings/ancestor_embedding_df.csv"
    script:
        "scripts/generate_ancestor_embeddings.py"


rule generate_mutations:
    input:
        # fasta=DATASET_PATH + "/{dataset_name}.fasta"
        fasta=DATASET_PATH + "/{dataset_name}_{datafile_name}.fasta"
    output:
        # generated_sequences="workflows/{dataset_name}/fasta/ungapped/{dataset_name}_{rep}.fasta",
        # mutation_positions="workflows/{dataset_name}/fasta/{dataset_name}_{rep}_mutationpos.txt"
        generated_sequences="workflows/{dataset_name}/{method_name}/fasta/ungapped/{datafile_name}_{rep}.fasta",
        mutation_positions="workflows/{dataset_name}/{method_name}/fasta/{datafile_name}_{rep}_mutationpos.txt"
    script:
        "scripts/generate_mutations.py"


rule train_logistic_regression:
    input:
        ancestor_embeddings="workflows/{dataset_name}/embeddings/ancestor_embedding_df.csv"
        # ancestor_embeddings="data/ancestor_embedding_df.csv"
    output:
        model_output="workflows/{dataset_name}/logregmodel/trained_logreg.pkl"
    script:
        "scripts/train_logistic_regression.py"


rule predict_logistic_regression:
    input:
        model="workflows/{dataset_name}/logregmodel/trained_logreg.pkl",
        embedding_df="workflows/{dataset_name}/{method_name}/embeddings/{datafile_name}_{rep}.csv",
    output:
        logreg_results="workflows/{dataset_name}/{method_name}/logregprediction/{datafile_name}_{rep}.csv"
    script:
        "scripts/predict_logistic_regression.py"

# rule remove_gaps:
#     input:
#         generated_sequences="workflows/{dataset_name}/fasta/ungapped/{dataset_name}_{rep}.fasta"
#     output:
#         generated_sequences_padded="workflows/{dataset_name}/fasta/padded/{dataset_name}_{rep}.fasta"
#     script:
#         "scripts/remove_gaps.py"
rule pad_gaps:
    input:
        # generated_sequences="workflows/{dataset_name}/fasta/ungapped/{dataset_name}_{rep}.fasta"
        generated_sequences="workflows/{dataset_name}/{method_name}/fasta/ungapped/{datafile_name}_{rep}.fasta"
    output:
        # generated_sequences_padded="workflows/{dataset_name}/fasta/padded/{dataset_name}_{rep}.fasta"
        generated_sequences_padded="workflows/{dataset_name}/{method_name}/fasta/padded/{datafile_name}_{rep}.fasta"
    script:
        "scripts/remove_gaps.py"


rule run_interproscan:
    input:
        # generated_sequences="workflows/{dataset_name}/fasta/ungapped/{dataset_name}_{rep}.fasta"
        generated_sequences="workflows/{dataset_name}/{method_name}/fasta/ungapped/{datafile_name}_{rep}.fasta"
    output:
        # interproscan_df="workflows/{dataset_name}/interproscan/{dataset_name}_{rep}.csv"
        interproscan_df="workflows/{dataset_name}/{method_name}/interproscan/{datafile_name}_{rep}.csv"
    params:
        interproscan_dir=INTERPROSCAN_DIR
    shell:
        """
        {params.interproscan_dir}/interproscan.sh \
        -dp \
        -i {input.generated_sequences} \
        -appl Gene3D,PRINTS \
        -f tsv -o {output.interproscan_df}
        """




rule generate_embeddings:
    input:
        # generated_sequences_padded="workflows/{dataset_name}/fasta/padded/{dataset_name}_{rep}.fasta",
        # generated_sequences_padded="workflows/{dataset_name}/fasta/padded/{datafile_name}_{rep}.fasta",
        input_sequences="workflows/{dataset_name}/{method_name}/fasta/padded/{datafile_name}_{rep}.fasta",
    output:
        #embedding_df="workflows/{dataset_name}/embeddings/{dataset_name}_{rep}.csv",
        embedding_df="workflows/{dataset_name}/{method_name}/embeddings/{datafile_name}_{rep}.csv",
    script:
        "scripts/generate_embeddings.py"

rule run_blast:
    input:
        # generated_sequences="workflows/{dataset_name}/fasta/ungapped/{dataset_name}_{rep}.fasta",
        generated_sequences="workflows/{dataset_name}/{method_name}/fasta/ungapped/{datafile_name}_{rep}.fasta",
    output:
        # blast_out="workflows/{dataset_name}/blast/blastresults_{dataset_name}_{rep}.tsv",
        blast_out="workflows/{dataset_name}/{method_name}/blast/blastresults_{datafile_name}_{rep}.tsv",
    shell:
        "blastp -db ../BLAST/NR_blastdb -query {input.generated_sequences} -out {output.blast_out} -evalue 1e-50 -num_threads 4 -max_target_seqs 10000 -outfmt '7 qseqid salltitles'"

rule parse_blast:
    input:
        # generated_sequences="workflows/{dataset_name}/fasta/ungapped/{dataset_name}_{rep}.fasta",
        # blast_out="workflows/{dataset_name}/blast/blastresults_{dataset_name}_{rep}.tsv",
        generated_sequences="workflows/{dataset_name}/{method_name}/fasta/ungapped/{datafile_name}_{rep}.fasta",
        blast_out="workflows/{dataset_name}/{method_name}/blast/blastresults_{datafile_name}_{rep}.tsv",
    output:
        # blast_df="workflows/{dataset_name}/blast/{dataset_name}_{rep}.csv",
        blast_df="workflows/{dataset_name}/{method_name}/blast/{datafile_name}_{rep}.csv",
    script:
        "scripts/parse_blast.py"

rule merge_outputs:
    input:
        # interproscan_df="workflows/{dataset_name}/interproscan/{dataset_name}_{rep}.csv",
        # embedding_df="workflows/{dataset_name}/embeddings/{dataset_name}_{rep}.csv",
        # blast_df="workflows/{dataset_name}/blast/{dataset_name}_{rep}.csv",
        # mutationfile="workflows/{dataset_name}/fasta/{dataset_name}_{rep}_mutationpos.txt"
        interproscan_df="workflows/{dataset_name}/{method_name}/interproscan/{datafile_name}_{rep}.csv",
        embedding_df="workflows/{dataset_name}/{method_name}/embeddings/{datafile_name}_{rep}.csv",
        blast_df="workflows/{dataset_name}/{method_name}/blast/{datafile_name}_{rep}.csv",
        mutationfile="workflows/{dataset_name}/{method_name}/fasta/{datafile_name}_{rep}_mutationpos.txt",
        logreg_results="workflows/{dataset_name}/{method_name}/logregprediction/{datafile_name}_{rep}.csv"

    output:
        # merged_df="workflows/{dataset_name}/merged/{dataset_name}_{rep}.csv",
        merged_df="workflows/{dataset_name}/{method_name}/merged/{datafile_name}_{rep}.csv",
    script:
        "scripts/merge_outputs.py"

rule plot_pca:
    input:
        embedding_df="workflows/{dataset_name}/{method_name}/embeddings/{datafile_name}_{rep}.csv",
        ancestor_embeddings="workflows/{dataset_name}/embeddings/ancestor_embedding_df.csv",
        predictions_df="workflows/{dataset_name}/{method_name}/results/{datafile_name}_{rep}.csv"
    output:
        plot_mutation="workflows/{dataset_name}/{method_name}/plots/{datafile_name}_{rep}_mutations.png",
        plot_prediction="workflows/{dataset_name}/{method_name}/plots/{datafile_name}_{rep}_predictions.png"
    script:
        "scripts/plot_pca.py"

rule plot_pca_grouped:
    input:
        embedding_df=lambda wildcards: expand(
            "workflows/{dataset_name}/{method_name}/embeddings/{datafile_name}_{rep}.csv", 
            dataset_name=wildcards.dataset_name,
            method_name=wildcards.method_name,
            datafile_name=wildcards.datafile_name,
            rep=range(int(wildcards.batch) * 5, (int(wildcards.batch) + 1) * 5)
        ),
        predictions_df=lambda wildcards: expand(
            "workflows/{dataset_name}/{method_name}/results/{datafile_name}_{rep}.csv", 
            dataset_name=wildcards.dataset_name,
            method_name=wildcards.method_name,
            datafile_name=wildcards.datafile_name,
            rep=range(int(wildcards.batch) * 5, (int(wildcards.batch) + 1) * 5)
        ),
        ancestor_embeddings="workflows/{dataset_name}/embeddings/ancestor_embedding_df.csv",
    output:
        plot_mutation="workflows/{dataset_name}/{method_name}/plots/{datafile_name}_{batch}_mutations.png",
        plot_prediction="workflows/{dataset_name}/{method_name}/plots/{datafile_name}_{batch}_predictions.png"
    script:
        "scripts/plot_pca_grouped.py"



        

rule gather_prediction_info:
    input:
        merged_df="workflows/{dataset_name}/{method_name}/merged/{datafile_name}_{rep}.csv"
    output:
        results_df="workflows/{dataset_name}/{method_name}/results/{datafile_name}_{rep}.csv"
    script:
        "scripts/gather_prediction_info.py"

rule plot_statistics:
    input:
        # expand("workflows/{dataset_name}/{method_name}/results/{datafile_name}_{rep}.csv", rep=range(1, NUM_REPLICATES + 1))
        lambda wildcards: expand(
            "workflows/{dataset_name}/{method_name}/results/{datafile_name}_{rep}.csv",
            dataset_name=wildcards.dataset_name,
            method_name=wildcards.method_name,
            datafile_name=wildcards.datafile_name,
            rep=range(1, NUM_REPLICATES + 1)
        )
    output:
        mutation_graphs="workflows/{dataset_name}/{method_name}/results/{datafile_name}_mutation.png",
        position_graphs="workflows/{dataset_name}/{method_name}/results/{datafile_name}_position.png",
        mutation_spread="workflows/{dataset_name}/{method_name}/results/{datafile_name}_mutationspread.png",
    script:
        "scripts/create_stats_plots.py"
